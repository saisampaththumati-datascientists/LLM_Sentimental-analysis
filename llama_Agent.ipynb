{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saisampaththumati-datascientists/LLM_Sentimental-analysis/blob/main/llama_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4533738c",
      "metadata": {
        "id": "4533738c"
      },
      "source": [
        "### Agents in the LangChain\n",
        "- what is Agents ?\n",
        "- ***Agents are systems that use LLMs as reasoning engines to determine which actions to take and the inputs to pass them.***\n",
        "- Utilize the langchain framework to deploy agents effectively for various business scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36c6deb7",
      "metadata": {
        "id": "36c6deb7"
      },
      "source": [
        "#### Library Required\n",
        "- langchain\n",
        "- openai\n",
        "- langchain-openai\n",
        "- langchain-experimental\n",
        "- langchainhub/ lanchchain hub has the all the model\n",
        "- duckduckgo_search/ for the web search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d1eb68e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d1eb68e",
        "outputId": "df5cd902-b272-4ddc-d958-d548d428c892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.8/328.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.0/990.0 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.7/202.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.8/384.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai==1.36.1 \\\n",
        "                langchain==0.2.10\\\n",
        "                langchain-openai==0.1.17 \\\n",
        "                langchainhub==0.1.20 \\\n",
        "                langchain-experimental==0.0.62 \\\n",
        "                duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aaaab9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aaaab9d",
        "outputId": "06faae17-167c-4210-891f-5831a114f2e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain-experimental\n",
            "Version: 0.0.62\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: langchain-community, langchain-core\n",
            "Required-by: \n",
            "---\n",
            "Name: duckduckgo_search\n",
            "Version: 5.3.1b1\n",
            "Summary: Search for words, documents, images, news, maps and text translation using the DuckDuckGo.com search engine.\n",
            "Home-page: https://github.com/deedy5/duckduckgo_search\n",
            "Author: deedy5\n",
            "Author-email: \n",
            "License: MIT License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: click, httpx\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain\n",
            "Version: 0.2.10\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-community\n",
            "---\n",
            "Name: langchain-openai\n",
            "Version: 0.1.17\n",
            "Summary: An integration package connecting OpenAI and LangChain\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: langchain-core, openai, tiktoken\n",
            "Required-by: \n",
            "---\n",
            "Name: langchainhub\n",
            "Version: 0.1.20\n",
            "Summary: The LangChain Hub API client\n",
            "Home-page: \n",
            "Author: LangChain\n",
            "Author-email: support@langchain.dev\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: packaging, requests, types-requests\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show langchain-experimental duckduckgo_search langchain langchain-openai langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a121fb4",
      "metadata": {
        "id": "9a121fb4"
      },
      "source": [
        "**The project overview**\n",
        "- We are building the sql database\n",
        "- we are import the csv file in to the database\n",
        "- give the access to the gpt and use predefined agent acces the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe08f231",
      "metadata": {
        "id": "fe08f231"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sqlite3\n",
        "import os\n",
        "import pandas as pd\n",
        "from langchain import hub\n",
        "#from langchain_openai import ChatOpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import hub\n",
        "\n",
        "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
        "from langchain.tools import DuckDuckGoSearchRun, yahoo_finance_news\n",
        "\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "from langchain_community.agent_toolkits import create_sql_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b7449f",
      "metadata": {
        "id": "c9b7449f"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb54a24b",
      "metadata": {
        "id": "fb54a24b"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY']= 'kEuHJVrjTTy6o2EQ7F3oT3BlbkFJBbAZeYTAY4IhhO3MrFD1' # Write your api key here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb9f1d7",
      "metadata": {
        "id": "feb9f1d7"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_API_KEY'] = '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'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1980c376",
      "metadata": {
        "id": "1980c376"
      },
      "source": [
        "### Import the mitral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eedbd1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eedbd1d",
        "outputId": "9e6e7d1c-7ade-4d53-9441-32646e66dbf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.88.tar.gz (63.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.88-cp310-cp310-linux_x86_64.whl size=3358616 sha256=2a217a9dfc490b3e25d1cc2de28b8a0ea99459b328ac960845f6df92531cb5d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/c5/5a/48a19d744c67b8886c5d644b8a01dff6c93fec6d34fa732349\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.88\n",
            "Collecting huggingface\n",
            "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: huggingface\n",
            "Successfully installed huggingface-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1dbad45",
      "metadata": {
        "id": "f1dbad45"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aefa8b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "9b4acd1be48241099a3e88c36a0cd1cd",
            "270439db62074b57a435ed4ef3f3088c",
            "118c2e7ef64d4c20aa57fb7f9e8e5932",
            "e7e04e176ea143cf99508767c17ef790",
            "e31592526e874673a74c90e7bf045856",
            "779b726f83a04a8b8f8327576cffa7c5",
            "8ba4ff76532c4836a3ba3050ab3cfa23",
            "d914106a895c4cc0a0b19d1287e86665",
            "87bcbe4a78ca43b3b429c5bfcd01e0a5",
            "93c67ccb7da94ad29ca652946d0d5c23",
            "158e825a561d4785a166f2a06ed93b25"
          ]
        },
        "id": "9aefa8b5",
        "outputId": "ad7fc8c2-8de1-46ae-c579-75b448b33e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mistral-7b-instruct-v0.2.Q5_K_M.gguf:   0%|          | 0.00/5.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b4acd1be48241099a3e88c36a0cd1cd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "file_version=\"mistral-7b-instruct-v0.2.Q5_K_M.gguf\"\n",
        "model_path= hf_hub_download(repo_id=model_name,filename=file_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c37fae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8c37fae",
        "outputId": "cd153a80-f5e9-45c4-a5bf-ead81acbe232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q5_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 3\n",
            "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 6016\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   752.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  752.00 MiB, K (f16):  376.00 MiB, V (f16):  376.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   419.76 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ],
      "source": [
        "mistral_model=Llama(model_path=model_path,n_batch=512,n_ctx=6000,n_gpu_layers=43,n_threads=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780c2152",
      "metadata": {
        "id": "780c2152"
      },
      "outputs": [],
      "source": [
        "mistral_model.verbose=False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999f0ad7",
      "metadata": {
        "id": "999f0ad7"
      },
      "source": [
        "### Notes:Custom Agent\n",
        "- The Tool classes from the Langchain raped around the Custom Agent classes around these tools. Each Agent follows a clearly defined prompt that guide tool useage.\n",
        "\n",
        "#### Python Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5802a46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5802a46",
        "outputId": "2d010850-21e7-4768-dff3-931fbbba1bcf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'PythonREPL',\n",
              " 'description': 'Simulates a standalone Python REPL.',\n",
              " 'type': 'object',\n",
              " 'properties': {'_globals': {'title': ' Globals', 'type': 'object'},\n",
              "  '_locals': {'title': ' Locals', 'type': 'object'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "python_repl=PythonREPL()\n",
        "python_repl.schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "429599c5",
      "metadata": {
        "id": "429599c5"
      },
      "outputs": [],
      "source": [
        "python_tool=Tool(\n",
        "name=\"python_repl\",\n",
        "description=\"The python shell is used for the excute the python command. Input should be in pythone code\",\n",
        "func=python_repl.run,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c2b18c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "92c2b18c",
        "outputId": "f5ab7508-366d-4c81-bcb2-63740fc88922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi this is my first agent\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "python_tool.invoke(\"print('Hi this is my first agent')\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0837017",
      "metadata": {
        "id": "d0837017"
      },
      "outputs": [],
      "source": [
        "# create the Tool promopt\n",
        "tool_prompt=\"\"\"\n",
        "[INST]Creat the pthon code for the following task\n",
        "{user_task}\n",
        "Instruction:\n",
        "output only code. Don't include anything mode.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e5eeb29",
      "metadata": {
        "id": "0e5eeb29"
      },
      "outputs": [],
      "source": [
        "task=\"Convert 39 degrees Celsius to Farenheit.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca8b8d3",
      "metadata": {
        "id": "5ca8b8d3"
      },
      "outputs": [],
      "source": [
        "response=mistral_model(\n",
        "prompt=tool_prompt.format(user_task=task),\n",
        "temperature=0,\n",
        "echo=False,\n",
        "repeat_penalty=1.2,\n",
        "max_tokens=200,\n",
        "\n",
        "top_k=40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a5f0ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35a5f0ad",
        "outputId": "2d6ce4ba-a52b-41f6-9c2f-6798e1e58f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "celsius = 39\n",
            "fahrenheit = (celsius * 1.8) + 32\n",
            "print(int(fahrenheit))\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "output=response[\"choices\"][0][\"text\"].strip()\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f1b764",
      "metadata": {
        "id": "39f1b764"
      },
      "outputs": [],
      "source": [
        "start_index=output.rfind('```python')\n",
        "end_index=output.rfind('```')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e9cda6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34e9cda6",
        "outputId": "cc3627f1-fc9f-405c-d112-c21a38710501"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 80)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "start_index,end_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b94a52ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "b94a52ec",
        "outputId": "5f7d72d1-e5dd-4ba8-ef92-8f34fbda640a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncelsius = 39\\nfahrenheit = (celsius * 1.8) + 32\\nprint(int(fahrenheit))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "output[start_index+9:end_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35186013",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "35186013",
        "outputId": "05f8bfb6-7d3e-4c40-ad92-cb41a635c753"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'102\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "python_tool.invoke(output[start_index+9:end_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074bcdf2",
      "metadata": {
        "id": "074bcdf2"
      },
      "source": [
        "#### Create the function combing the all the thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf105705",
      "metadata": {
        "id": "bf105705"
      },
      "outputs": [],
      "source": [
        "class pythonAgent:\n",
        "    \"\"\"\n",
        "    The Agent that can be used to excute the python repl.\n",
        "    Args:\n",
        "    llm(LLm):Take the llm mode\n",
        "    tool(Tool):tool to use\n",
        "    \"\"\"\n",
        "    def __init__(self,llm,tool):\n",
        "        self.llm=llm\n",
        "        self.tool=tool\n",
        "        self.tool_prompt_template=\"\"\"[INST]\n",
        "        Create the python code for the following question\n",
        "        {user_input}\n",
        "        Instrauction:\n",
        "        return only the code. Don't provide any other additional Details.[/INST]\n",
        "        \"\"\"\n",
        "        self.script=None\n",
        "    def run(self, user_task):\n",
        "        response = self.llm(\n",
        "        prompt=self.tool_prompt_template.format(user_input=user_task),\n",
        "        max_tokens=256,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        echo=False\n",
        "    )\n",
        "        output=response[\"choices\"][0][\"text\"].strip()\n",
        "        start_index=output.rfind(\"```python\")\n",
        "        end_index=output.rfind(\"```\")\n",
        "        python_index=(output[start_index+9:end_index])\n",
        "        self.script=python_index\n",
        "        try:\n",
        "            result=self.tool.invoke(python_index)\n",
        "        except Exception as e:\n",
        "            result=str(e)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5037ca85",
      "metadata": {
        "id": "5037ca85"
      },
      "outputs": [],
      "source": [
        "python_agent = pythonAgent(mistral_model, python_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0157da2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0157da2a",
        "outputId": "3e0fb101-155d-45ad-c896-f92625656145"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tool(name='python_repl', description='The python shell is used for the excute the python command. Input should be in pythone code', func=<bound method PythonREPL.run of PythonREPL(globals={'__builtins__': {'__name__': 'builtins', '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\", '__package__': '', '__loader__': <class '_frozen_importlib.BuiltinImporter'>, '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'), '__build_class__': <built-in function __build_class__>, '__import__': <built-in function __import__>, 'abs': <built-in function abs>, 'all': <built-in function all>, 'any': <built-in function any>, 'ascii': <built-in function ascii>, 'bin': <built-in function bin>, 'breakpoint': <built-in function breakpoint>, 'callable': <built-in function callable>, 'chr': <built-in function chr>, 'compile': <built-in function compile>, 'delattr': <built-in function delattr>, 'dir': <built-in function dir>, 'divmod': <built-in function divmod>, 'eval': <built-in function eval>, 'exec': <built-in function exec>, 'format': <built-in function format>, 'getattr': <built-in function getattr>, 'globals': <built-in function globals>, 'hasattr': <built-in function hasattr>, 'hash': <built-in function hash>, 'hex': <built-in function hex>, 'id': <built-in function id>, 'input': <bound method Kernel.raw_input of <google.colab._kernel.Kernel object at 0x7d4c3eb14a90>>, 'isinstance': <built-in function isinstance>, 'issubclass': <built-in function issubclass>, 'iter': <built-in function iter>, 'aiter': <built-in function aiter>, 'len': <built-in function len>, 'locals': <built-in function locals>, 'max': <built-in function max>, 'min': <built-in function min>, 'next': <built-in function next>, 'anext': <built-in function anext>, 'oct': <built-in function oct>, 'ord': <built-in function ord>, 'pow': <built-in function pow>, 'print': <built-in function print>, 'repr': <built-in function repr>, 'round': <built-in function round>, 'setattr': <built-in function setattr>, 'sorted': <built-in function sorted>, 'sum': <built-in function sum>, 'vars': <built-in function vars>, 'None': None, 'Ellipsis': Ellipsis, 'NotImplemented': NotImplemented, 'False': False, 'True': True, 'bool': <class 'bool'>, 'memoryview': <class 'memoryview'>, 'bytearray': <class 'bytearray'>, 'bytes': <class 'bytes'>, 'classmethod': <class 'classmethod'>, 'complex': <class 'complex'>, 'dict': <class 'dict'>, 'enumerate': <class 'enumerate'>, 'filter': <class 'filter'>, 'float': <class 'float'>, 'frozenset': <class 'frozenset'>, 'property': <class 'property'>, 'int': <class 'int'>, 'list': <class 'list'>, 'map': <class 'map'>, 'object': <class 'object'>, 'range': <class 'range'>, 'reversed': <class 'reversed'>, 'set': <class 'set'>, 'slice': <class 'slice'>, 'staticmethod': <class 'staticmethod'>, 'str': <class 'str'>, 'super': <class 'super'>, 'tuple': <class 'tuple'>, 'type': <class 'type'>, 'zip': <class 'zip'>, '__debug__': True, 'BaseException': <class 'BaseException'>, 'Exception': <class 'Exception'>, 'TypeError': <class 'TypeError'>, 'StopAsyncIteration': <class 'StopAsyncIteration'>, 'StopIteration': <class 'StopIteration'>, 'GeneratorExit': <class 'GeneratorExit'>, 'SystemExit': <class 'SystemExit'>, 'KeyboardInterrupt': <class 'KeyboardInterrupt'>, 'ImportError': <class 'ImportError'>, 'ModuleNotFoundError': <class 'ModuleNotFoundError'>, 'OSError': <class 'OSError'>, 'EnvironmentError': <class 'OSError'>, 'IOError': <class 'OSError'>, 'EOFError': <class 'EOFError'>, 'RuntimeError': <class 'RuntimeError'>, 'RecursionError': <class 'RecursionError'>, 'NotImplementedError': <class 'NotImplementedError'>, 'NameError': <class 'NameError'>, 'UnboundLocalError': <class 'UnboundLocalError'>, 'AttributeError': <class 'AttributeError'>, 'SyntaxError': <class 'SyntaxError'>, 'IndentationError': <class 'IndentationError'>, 'TabError': <class 'TabError'>, 'LookupError': <class 'LookupError'>, 'IndexError': <class 'IndexError'>, 'KeyError': <class 'KeyError'>, 'ValueError': <class 'ValueError'>, 'UnicodeError': <class 'UnicodeError'>, 'UnicodeEncodeError': <class 'UnicodeEncodeError'>, 'UnicodeDecodeError': <class 'UnicodeDecodeError'>, 'UnicodeTranslateError': <class 'UnicodeTranslateError'>, 'AssertionError': <class 'AssertionError'>, 'ArithmeticError': <class 'ArithmeticError'>, 'FloatingPointError': <class 'FloatingPointError'>, 'OverflowError': <class 'OverflowError'>, 'ZeroDivisionError': <class 'ZeroDivisionError'>, 'SystemError': <class 'SystemError'>, 'ReferenceError': <class 'ReferenceError'>, 'MemoryError': <class 'MemoryError'>, 'BufferError': <class 'BufferError'>, 'Warning': <class 'Warning'>, 'UserWarning': <class 'UserWarning'>, 'EncodingWarning': <class 'EncodingWarning'>, 'DeprecationWarning': <class 'DeprecationWarning'>, 'PendingDeprecationWarning': <class 'PendingDeprecationWarning'>, 'SyntaxWarning': <class 'SyntaxWarning'>, 'RuntimeWarning': <class 'RuntimeWarning'>, 'FutureWarning': <class 'FutureWarning'>, 'ImportWarning': <class 'ImportWarning'>, 'UnicodeWarning': <class 'UnicodeWarning'>, 'BytesWarning': <class 'BytesWarning'>, 'ResourceWarning': <class 'ResourceWarning'>, 'ConnectionError': <class 'ConnectionError'>, 'BlockingIOError': <class 'BlockingIOError'>, 'BrokenPipeError': <class 'BrokenPipeError'>, 'ChildProcessError': <class 'ChildProcessError'>, 'ConnectionAbortedError': <class 'ConnectionAbortedError'>, 'ConnectionRefusedError': <class 'ConnectionRefusedError'>, 'ConnectionResetError': <class 'ConnectionResetError'>, 'FileExistsError': <class 'FileExistsError'>, 'FileNotFoundError': <class 'FileNotFoundError'>, 'IsADirectoryError': <class 'IsADirectoryError'>, 'NotADirectoryError': <class 'NotADirectoryError'>, 'InterruptedError': <class 'InterruptedError'>, 'PermissionError': <class 'PermissionError'>, 'ProcessLookupError': <class 'ProcessLookupError'>, 'TimeoutError': <class 'TimeoutError'>, 'open': <built-in function open>, 'copyright': Copyright (c) 2001-2023 Python Software Foundation.\n",
              "All Rights Reserved.\n",
              "\n",
              "Copyright (c) 2000 BeOpen.com.\n",
              "All Rights Reserved.\n",
              "\n",
              "Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
              "All Rights Reserved.\n",
              "\n",
              "Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
              "All Rights Reserved., 'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
              "    for supporting Python development.  See www.python.org for more information., 'license': Type license() to see the full license text, 'help': Type help() for interactive help, or help(object) for help about object., '__IPYTHON__': True, 'display': <function display at 0x7d4c46b84ca0>, 'execfile': <function execfile at 0x7d4c2dc86830>, 'runfile': <function runfile at 0x7d4c2dae8430>, 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7d4c3eb14b20>>}}, locals={'celsius': 39, 'fahrenheit': 102.2})>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "python_agent.tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3454dac3",
      "metadata": {
        "id": "3454dac3"
      },
      "outputs": [],
      "source": [
        "result = python_agent.run(\"Convert 39 degrees Celsius to Farenheit.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d81dd92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d81dd92",
        "outputId": "c1a8eef0-59ac-4e65-faff-9b6f3f96b142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39.0°C is equal to 102.20°F\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f1e71f",
      "metadata": {
        "id": "a9f1e71f"
      },
      "outputs": [],
      "source": [
        "class PythonAgent:\n",
        "  \"\"\"\n",
        "  An agent that can be used to execute tasks using the Python repl.\n",
        "\n",
        "  Args:\n",
        "    llm (LLM): The LLM to use.\n",
        "    tool (Tool): The tool to use.\n",
        "  \"\"\"\n",
        "  def __init__(self, llm, tool):\n",
        "    self.llm = llm\n",
        "    self.tool = tool\n",
        "    self.tool_prompt_template = \"\"\"[INST]\n",
        "    Create Python code to execute the following task:\n",
        "    {user_task}\n",
        "    Output only the code. Do not include anything else in your output.\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "    self.script = None # Place holder for the Python script that will be generated during runtime\n",
        "\n",
        "  def run(self, user_task):\n",
        "    response = self.llm(\n",
        "        prompt=self.tool_prompt_template.format(user_task=user_task),\n",
        "        max_tokens=256,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        echo=False\n",
        "    )\n",
        "    output = response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "    # Parse Output\n",
        "    start_index = output.find('```python')\n",
        "    end_index = output.rfind('```')\n",
        "\n",
        "    python_script = output[start_index+9:end_index]\n",
        "\n",
        "    self.script = python_script\n",
        "\n",
        "    # Execute script\n",
        "\n",
        "    try:\n",
        "      result = self.tool.invoke(python_script)\n",
        "    except Exception as e:\n",
        "      result = str(e)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46e0737",
      "metadata": {
        "id": "a46e0737"
      },
      "outputs": [],
      "source": [
        "python_agent = PythonAgent(mistral_model, python_tool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ab32839",
      "metadata": {
        "id": "4ab32839"
      },
      "outputs": [],
      "source": [
        "result = python_agent.run(\"Convert 39 degrees Celsius to Farenheit.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5418145",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5418145",
        "outputId": "0f678c17-8f29-4a82-c7e9-f513299a61c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102.2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3acf6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f3acf6f",
        "outputId": "43138b19-b9ef-4dfd-9a8f-85ab0a32a1f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "celsius = 39.0\n",
            "fahrenheit = (celsius * 9 / 5) + 32\n",
            "print(round(fahrenheit, 1))\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(python_agent.script)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "961d23dc",
      "metadata": {
        "id": "961d23dc"
      },
      "outputs": [],
      "source": [
        " math_problem = \"A town has a population of 12,000.\" \\\n",
        "               \" The population increases by 12% per year.\" \\\n",
        "               \" What will be the population after 3 years?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91945f64",
      "metadata": {
        "id": "91945f64"
      },
      "outputs": [],
      "source": [
        "result_1=python_agent.run(math_problem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9d44a5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9d44a5a",
        "outputId": "9c6f19c8-b24b-403a-bfab-4cb7f3ff5e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16859\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(result_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518397a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "518397a1",
        "outputId": "c7ed14b9-6c8e-4575-ffc5-aac43ba513ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "population = 12000\n",
            "growth_rate = 0.12\n",
            "num_years = 3\n",
            "population *= (1 + growth_rate) ** num_years\n",
            "print(int(population))\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(python_agent.script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e43cd02b",
      "metadata": {
        "id": "e43cd02b"
      },
      "outputs": [],
      "source": [
        "math_problem_1=\"A man is climbing up a mountain which is inclined. He has to travel 100 km to reach the top of the mountain. Every day He climbs up 2 km forward in the day time. Exhausted, he then takes rest there at night time. At night, while he is asleep, he slips down 1 km backwards because the mountain is inclined. Then how many days does it take him to reach the mountain top? \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c3990f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c3990f6",
        "outputId": "f8363222-ae8b-40e4-aecf-13abc3f0103a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result_1=python_agent.run(math_problem_1)\n",
        "print(result_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e899b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20e899b5",
        "outputId": "6d08ec4d-44ce-4a99-85f1-19d412140cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SyntaxError(\"'(' was never closed\", ('<string>', 3, 11, 'eq = sp.Eq(sp.Subs(sp.QuadaticForm(6, 1, -1)(x, y) + 12, 0)', 3, 0))\n"
          ]
        }
      ],
      "source": [
        "math_problem=\"Which is a point on the line 6x – y = 12?\"\n",
        "result_1=python_agent.run(math_problem)\n",
        "print(result_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64bd879e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64bd879e",
        "outputId": "f0d3efee-1c68-41ba-b544-2cb1f5b9f517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "import sympy as sp\n",
            "x, y = sp.symbols('x y')\n",
            "eq = sp.Eq(sp.Subs(sp.QuadaticForm(6, 1, -1)(x, y) + 12, 0)\n",
            "sol = sp.solve(eq, (x, y))\n",
            "print(sol[0])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(python_agent.script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8af6425",
      "metadata": {
        "id": "a8af6425"
      },
      "outputs": [],
      "source": [
        "math_problem=\"A graphic designer creates an image for a t-shirt that measures 8 inches tall and 10 inches wide. She can use this image for small and medium t-shirts, but she must increase the size for a large t-shirt. She wants her final image for the large t-shirts to measure 9 inches tall and 11.25 inches wide. At what percent must she print out the image to obtain these measurements?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d57fccc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d57fccc",
        "outputId": "8923c879-67b7-4dbe-ade8-8d4c025b874f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "# Given dimensions of original image (inches)\n",
            "width_original, height_original = 10, 8\n",
            "\n",
            "# Desired dimensions for large t-shirt (inches)\n",
            "width_large, height_larger = 11.25, 9\n",
            "\n",
            "# Calculate percentage increase in width and height\n",
            "percentage_width = ((width_large - width_original) / width_original) * 100\n",
            "percentage_height = ((height_larger - height_origin) / height_original) * 100\n",
            "\n",
            "print(f\"Percentage increase for width: {percentage_width:.2f}%\")\n",
            "print(f\"Percentage increase for height: {percentage_height:.2f}%\")\n",
            "\n",
            "NameError(\"name 'height_origin' is not defined\")\n"
          ]
        }
      ],
      "source": [
        "result_1=python_agent.run(math_problem)\n",
        "print(python_agent.script)\n",
        "print(result_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3019c0e",
      "metadata": {
        "id": "d3019c0e"
      },
      "source": [
        "### Search engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb499638",
      "metadata": {
        "id": "eb499638"
      },
      "outputs": [],
      "source": [
        "duck_duck_go=DuckDuckGoSearchRun()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af62fa78",
      "metadata": {
        "id": "af62fa78"
      },
      "outputs": [],
      "source": [
        "search=Tool(\n",
        "name=\"duck_duck_go\",\n",
        "description=\"A interface to duckduckgo search engine. The userinput should be string \",\n",
        "func=duck_duck_go.run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51c4d0a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51c4d0a5",
        "outputId": "fbf96662-3700-4a9c-a81a-7b0fd3a1cbd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tool(name='duck_duck_go', description='A interface to duckduckgo search engine. The userinput should be string ', func=<bound method BaseTool.run of DuckDuckGoSearchRun()>)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa49510a",
      "metadata": {
        "id": "aa49510a"
      },
      "outputs": [],
      "source": [
        "react_prompt_template = \"\"\"[INST]\n",
        "Answer the following questions as best you can. You have access to the following tool:\n",
        "\n",
        "search_tool: An interface to the DuckDuckGo search engine. Input should be a string.\n",
        "\n",
        "The way you use the tool is by specifying a json blob.\n",
        "Specifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\n",
        "\n",
        "The only values that should be in the \"action\" field are:\n",
        "search_tool: A function to search the DuckDuckGo search engine\n",
        "\n",
        "The $JSON_BLOB should only contain a SINGLE action and MUST be formatted as markdown, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\n",
        "\n",
        "```\n",
        "{{\n",
        "  \"action\": $TOOL_NAME,\n",
        "  \"action_input\": $INPUT\n",
        "}}\n",
        "```\n",
        "Make sure to have the $INPUT in the right format for the tool you are using, and do not put variable names as input if you can find the right values.\n",
        "\n",
        "ALWAYS use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about one action to take. Only one action at a time in this format:\n",
        "Action:\n",
        "```\n",
        "$JSON_BLOB\n",
        "```\n",
        "Question: {user_question}\n",
        "\n",
        "[/INST]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a6336d3",
      "metadata": {
        "id": "3a6336d3"
      },
      "outputs": [],
      "source": [
        "user_question=\"what is the current tesla stockprice and the sticker code\"\n",
        "response= mistral_model(\n",
        "prompt=react_prompt_template.format(user_question=user_question),\n",
        "temperature=0,\n",
        "max_tokens=500,\n",
        "top_k=40,\n",
        "top_p=0.82,\n",
        "echo=False,\n",
        "repeat_penalty=1.2)\n",
        "output=response[\"choices\"][0][\"text\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6877b366",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6877b366",
        "outputId": "83ea14f0-c80b-4df1-c159-e29fa32da1e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "output.rfind('```json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1af5ecd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1af5ecd7",
        "outputId": "9f03d5ab-33a3-4b36-c14b-a6609cd55e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to search for \"tesla stock price\" and \"tesla ticker symbol\" on DuckDuckGo.\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"search_tool\",\n",
            "  \"action_input\": \"tesla stock price AND ticker symbol\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe6f83a",
      "metadata": {
        "id": "3fe6f83a"
      },
      "outputs": [],
      "source": [
        "class SearchAgent:\n",
        "  \"\"\"\n",
        "  This agent will use the search tool to answer questions.\n",
        "  \"\"\"\n",
        "  def __init__(self, llm, tool, prompt_template):\n",
        "    self.llm = llm\n",
        "    self.tool = tool\n",
        "    self.tool_prompt_template = prompt_template\n",
        "    self.verbose = True\n",
        "\n",
        "  def run(self, question):\n",
        "    response = self.llm(\n",
        "        prompt=self.tool_prompt_template.format(user_question=question),\n",
        "        max_tokens=4096,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    output = response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "    if self.verbose:\n",
        "      print(output)\n",
        "\n",
        "    # Parse Output\n",
        "    start_index = output.find('```json')\n",
        "    end_index = output.rfind('```')\n",
        "\n",
        "    tool_choice = json.loads(output[start_index+7:end_index])\n",
        "    tool_input = tool_choice[\"action_input\"]\n",
        "\n",
        "    # Execute search\n",
        "    try:\n",
        "      search_result = self.tool.invoke(tool_input)\n",
        "    except Exception as e:\n",
        "      search_result = \"Cannot find relevant information\"\n",
        "\n",
        "    # Answer the question\n",
        "\n",
        "    answering_prompt = f\"\"\"\n",
        "    [INST]Given the context:\\n {search_result}, answer the following question based on this context:\\n {question}.\n",
        "    Do not include information about the context.[/INST]\"\"\"\n",
        "\n",
        "    final_response = self.llm(\n",
        "        prompt=answering_prompt,\n",
        "        max_tokens=4096,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    return final_response[\"choices\"][0][\"text\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78413b97",
      "metadata": {
        "id": "78413b97"
      },
      "outputs": [],
      "source": [
        "class Search_Agent:\n",
        "    def __init__(self,llm,tool,prompt):\n",
        "        self.llm=llm\n",
        "        self.tool=tool\n",
        "        self.prompt_template=prompt\n",
        "        self.verbose=True\n",
        "    def run(self,user_input):\n",
        "        response=self.llm(\n",
        "        prompt=self.prompt_template.format(user_question=user_input),\n",
        "        temperature=0,\n",
        "        max_tokens=4096,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        echo=False\n",
        "        )\n",
        "        output= response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "        if self.verbose:\n",
        "            print(output)\n",
        "        #parse output\n",
        "        start_index=output.rfind('```json')\n",
        "        end_index=output.rfind('```')\n",
        "        tool_choice=json.loads(output[start_index+7:end_index])\n",
        "        tool_index=tool_choice[\"action_input\"]\n",
        "        try:\n",
        "            search_result=self.tool.invoke(tool_index)\n",
        "        except Exception as e:\n",
        "            search_result=\"can't find the relevant information\"\n",
        "\n",
        "        answer_prompt=\"\"\"\n",
        "        [INST]Given the context:\\n {search_result}, answer the following question based on this context:\\n {question}.\n",
        "        Do not include information about the context.[/INST]\"\"\"\n",
        "        final_response=self.llm(\n",
        "        prompt=answer_prompt,\n",
        "        max_tokens=4096,\n",
        "        temperature=0,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        echo=False\n",
        "        )\n",
        "        return final_response[\"choices\"][0][\"text\"].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83cbbff8",
      "metadata": {
        "id": "83cbbff8"
      },
      "outputs": [],
      "source": [
        "search_agent = Search_Agent(mistral_model, search, react_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a46ef7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "c1a46ef7",
        "outputId": "e3a5c567-71b4-4f69-fc07-cce1421461aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to search for the latest stock price of Apple (AAPL) on a reliable financial news website or finance aggregator.\n",
            "\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"search_tool\",\n",
            "  \"action_input\": \"latest apple stock price\"\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm unable to provide an answer without having access to the given search result or question within the context. Please ensure both pieces of information are provided for me to help you with your query.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "search_agent.run(user_input=\"What is the latest stock price of AAPL?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c20b1c",
      "metadata": {
        "id": "d7c20b1c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c54ea356",
      "metadata": {
        "id": "c54ea356"
      },
      "outputs": [],
      "source": [
        "search_agent = SearchAgent(mistral_model, search, react_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f884cd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "0f884cd9",
        "outputId": "9fcbe000-7e4a-4011-b5b3-6d83e3f5cbd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to search for the latest stock price of Apple (AAPL) on a reliable financial news website or finance aggregator.\n",
            "\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"search_tool\",\n",
            "  \"action_input\": \"latest apple stock price\"\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The latest record-high closing stock price for AAPL is $237.23 (as mentioned in the first line of the context). However, since I'm supposed to answer without including any contextual information, I can only say that I don't have access to real-time or specific stock prices and you should check a reliable financial news source for the most current price.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "search_agent.run(question=\"What is the latest stock price of AAPL?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d890878",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "5d890878",
        "outputId": "41b9282e-10b3-4e50-a727-eec16e49008c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I will search the web for information on \"Rag in llms\" using DuckDuckGo.\n",
            "\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"search_tool\",\n",
            "  \"action_input\": \"Rag in llms\"\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"RAG (Retrieval Augmented Generation) is a technique used with Language Learning Models (LLMs) to enhance their performance by retrieving relevant facts from an external knowledge base and using them to generate accurate responses. It combines LLM's ability to understand language context with the power of information retrieval systems, resulting in more precise and consistent answers for users. RAG is particularly useful in enterprise applications where up-to-date and accurate information is crucial.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "search_agent.run(question=\"what is Rag in llms? where it is use?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ba7952",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7ba7952",
        "outputId": "1322ab9d-88e5-4536-ad79-7e4d9eadedcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tool(name='duck_duck_go', description='A interface to duckduckgo search engine. The userinput should be string ', func=<bound method BaseTool.run of DuckDuckGoSearchRun()>)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "search_agent.tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "853155ae",
      "metadata": {
        "id": "853155ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4f0d31b4",
      "metadata": {
        "id": "4f0d31b4"
      },
      "source": [
        "### Creating search react Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d483e3ac",
      "metadata": {
        "id": "d483e3ac"
      },
      "outputs": [],
      "source": [
        "class SearchReactAgent:\n",
        "    def __init__(self,llm,tool,prompt):\n",
        "        self.llm=llm\n",
        "        self.tool=tool\n",
        "        self.react_prompt_template=prompt\n",
        "        self.memory=''\n",
        "        self.observation=None\n",
        "        self.final_answer=None\n",
        "    def run(self,user_message):\n",
        "        response=self.llm(\n",
        "        prompt=self.react_prompt_template.format(user_question=user_message),\n",
        "        temperature=0,\n",
        "        max_tokens=2000,\n",
        "        echo=False,\n",
        "        top_p=0.85,\n",
        "        repeat_penalty=1.2)\n",
        "        output=response[\"choices\"][0][\"text\"].strip()\n",
        "        start_index=output.rfind('```json')\n",
        "        end_index=output.rfind('```')\n",
        "        try:\n",
        "            tool_choice = json.loads(output[start_index+7:end_index])\n",
        "            tool_index=tool_choice[\"action_input\"]\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "        try:\n",
        "            search_result=self.tool.invoke(tool_index)\n",
        "        except Exception as e:\n",
        "            search_result=\"Not found any Info\"\n",
        "        answering_prompt = f\"\"\"\n",
        "        [INST]Given the context:\\n {search_result}, answer the following question based on this context:\\n {user_message}.\n",
        "        Do not include information about the context.[/INST]\"\"\"\n",
        "        response=self.llm(\n",
        "        prompt=answering_prompt,\n",
        "        temperature=0,\n",
        "        max_tokens=2000,\n",
        "        repeat_penalty=1.2,\n",
        "        top_p=0.85,\n",
        "        echo=False)\n",
        "        self.observation=response[\"choices\"][0][\"text\"].strip()\n",
        "        self.memory += (output + '\\n' + f\"Observation: {self.observation}\" + '\\n')\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6453e94",
      "metadata": {
        "id": "b6453e94"
      },
      "outputs": [],
      "source": [
        "search=SearchReactAgent(mistral_model,search,react_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d76103",
      "metadata": {
        "id": "a1d76103"
      },
      "outputs": [],
      "source": [
        "search.run(user_message=\"what is today day and get the stock details of apple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a23b959",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a23b959",
        "outputId": "d2e37e24-d533-4c9c-9c80-96aafd494580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find out today's date first, then use that information to search for Apple's stock details.\n",
            "\n",
            "Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"search_tool\",\n",
            "  \"action_input\": \"what is today's date\"\n",
            "}\n",
            "```\n",
            "After receiving the current date from the previous query, I will use it to search for Apple's stock details using DuckDuckGo.\n",
            "\n",
            "Next Action:\n",
            "```json\n",
            "{\n",
            "  \"action\": \"search_tool\",\n",
            "  \"action_input\": \"Apple Inc. stock AAPL\"\n",
            "}\n",
            "```\n",
            "Observation: I cannot determine today's date without access to current information. However, according to the context provided, the Apple Inc. (AAPL) stock price as of August 9 was $216.24.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(search.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35219afb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "35219afb",
        "outputId": "6c913c9c-e02b-4537-b1fb-b02ca58eb8e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I cannot determine today's date without access to current information. However, according to the context provided, the Apple Inc. (AAPL) stock price as of August 9 was $216.24.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "search.observation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a3b9233",
      "metadata": {
        "id": "1a3b9233"
      },
      "source": [
        "#### Agent finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fc7d1e",
      "metadata": {
        "id": "14fc7d1e"
      },
      "outputs": [],
      "source": [
        "class AgentFinish:\n",
        "    \"\"\"\n",
        "    This class will check if the agent has reached the final solution.\n",
        "    it access the memory of the agent whether it reached the end of the text\n",
        "    \"\"\"\n",
        "    def __init__(self,llm,agent):\n",
        "        self.llm=llm\n",
        "        self.agent=agent\n",
        "    def run(self,question):\n",
        "        checking_prompt = f\"\"\"\n",
        "    [INST]Given the following status of the thought process of a LLM agent:\\n {self.agent.memory},\n",
        "    verify if the observation of the agent is enough to answer the following question:\\n {question}.\n",
        "    Answer only as yes or no (single word). Do not include any other information.[/INST]\"\"\"\n",
        "        response=self.llm(\n",
        "        prompt=checking_prompt,\n",
        "        temperature=0,\n",
        "        top_p=0.90,\n",
        "        top_k=1.2,\n",
        "        repeat_penalty=1.2,\n",
        "        echo=False\n",
        "\n",
        "        )\n",
        "        finish_status=response[\"choices\"][0][\"text\"].strip().lower().replace(\".\",\" \")\n",
        "        return finish_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec1f3c74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ec1f3c74",
        "outputId": "62db11a5-5628-438b-9e60-9339183f2c42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"no  the agent was unable to determine today's date in the given context\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "finish_checker=AgentFinish(mistral_model,search)\n",
        "finish_checker.run(question=\"what is today day and get the stock details of apple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2dd060",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "5f2dd060",
        "outputId": "63e63c81-7d1d-47fe-f6f2-25dc55ce3558"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-0dfad05421a2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfinish_status\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msearch_react_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfinish_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAgentFinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmistral_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearch_react_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfinish_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinish_checker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"yes \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-a7754d027e5b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, user_message)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_answer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         response=self.llm(\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreact_prompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_question\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1789\u001b[0m             \u001b[0mResponse\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \"\"\"\n\u001b[0;32m-> 1791\u001b[0;31m         return self.create_completion(\n\u001b[0m\u001b[1;32m   1792\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1722\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCreateCompletionStreamResponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m         \u001b[0mcompletion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0mfinish_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0mmultibyte_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m   1209\u001b[0m             \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;31m# Eval and sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m                 token = self.sample(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             )\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0;31m# Save tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_past\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_cpp/_internals.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         return_code = llama_cpp.llama_decode(\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "user_input=\"what is Rag in llms? where it is use?\"\n",
        "finish_status= False\n",
        "search_react_agent=SearchReactAgent(mistral_model,search,react_prompt_template)\n",
        "\n",
        "while not finish_status:\n",
        "    search_react_agent.run(user_input)\n",
        "    finish_status=AgentFinish(mistral_model,search_react_agent)\n",
        "    finish_status=(finish_checker.run(user_input)==\"yes \")\n",
        "\n",
        "search_react_agent.final_answer = search_react_agent.observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f815460c",
      "metadata": {
        "id": "f815460c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b4acd1be48241099a3e88c36a0cd1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_270439db62074b57a435ed4ef3f3088c",
              "IPY_MODEL_118c2e7ef64d4c20aa57fb7f9e8e5932",
              "IPY_MODEL_e7e04e176ea143cf99508767c17ef790"
            ],
            "layout": "IPY_MODEL_e31592526e874673a74c90e7bf045856"
          }
        },
        "270439db62074b57a435ed4ef3f3088c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_779b726f83a04a8b8f8327576cffa7c5",
            "placeholder": "​",
            "style": "IPY_MODEL_8ba4ff76532c4836a3ba3050ab3cfa23",
            "value": "mistral-7b-instruct-v0.2.Q5_K_M.gguf: 100%"
          }
        },
        "118c2e7ef64d4c20aa57fb7f9e8e5932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d914106a895c4cc0a0b19d1287e86665",
            "max": 5131409696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87bcbe4a78ca43b3b429c5bfcd01e0a5",
            "value": 5131409696
          }
        },
        "e7e04e176ea143cf99508767c17ef790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93c67ccb7da94ad29ca652946d0d5c23",
            "placeholder": "​",
            "style": "IPY_MODEL_158e825a561d4785a166f2a06ed93b25",
            "value": " 5.13G/5.13G [00:29&lt;00:00, 240MB/s]"
          }
        },
        "e31592526e874673a74c90e7bf045856": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "779b726f83a04a8b8f8327576cffa7c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ba4ff76532c4836a3ba3050ab3cfa23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d914106a895c4cc0a0b19d1287e86665": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87bcbe4a78ca43b3b429c5bfcd01e0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93c67ccb7da94ad29ca652946d0d5c23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "158e825a561d4785a166f2a06ed93b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}